# ğŸ–ï¸ Sign Language Recognition System

**Computer Vision University Project**

## ğŸ“Œ Overview

This project implements a **Sign Language Recognition System** using **Computer Vision and Deep Learning**.
The system recognizes static sign language gestures in real time using a webcam and translates them into textual labels.

The project was developed as part of a **Computer Vision course university project**, focusing on building a complete and structured machine learning pipeline using a **dataset-based approach**.

---

## ğŸ¯ Project Objectives

* Build a sign language recognition system using a **pre-collected dataset**
* Extract meaningful hand features using **MediaPipe**
* Train a neural network for gesture classification
* Perform **real-time recognition** using a webcam
* Ensure consistent preprocessing between training and inference

---

## ğŸ§  System Pipeline

1. Load labeled sign language dataset
2. Extract hand landmarks from images
3. Convert landmarks into numerical feature vectors
4. Train a neural network classifier
5. Perform real-time prediction using webcam input

---

## ğŸ“‚ Dataset

* The project uses a **publicly available sign language dataset**
* Data is organized in a folder-based structure
* Each folder represents one sign label
* Images contain static hand gestures

ğŸ“Š **Dataset Link:**
ğŸ‘‰ *Add dataset link here*

---

## ğŸ› ï¸ Technologies Used

* **Python**
* **OpenCV**
* **MediaPipe**
* **TensorFlow / Keras**
* **NumPy**
* **Scikit-learn**

---

## ğŸ§© Feature Extraction

* MediaPipe Hands is used to detect one hand per image/frame
* 21 hand landmarks are extracted
* Each landmark contains (x, y, z) coordinates
* Total features per sample: **63 values**

This approach improves speed and robustness compared to using raw images.

---

## ğŸ—ï¸ Model Architecture

* Fully Connected Neural Network (MLP)
* Input layer: 63 features
* Hidden layers:

  * 128 neurons (ReLU)
  * 64 neurons (ReLU)
* Output layer: Softmax activation
* Optimizer: Adam
* Loss function: Categorical Cross-Entropy

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Install Dependencies

```bash
pip install opencv-python mediapipe numpy tensorflow scikit-learn
```

---

### 2ï¸âƒ£ Project Structure

```
New Project/
â”‚
â”œâ”€â”€ extract_landmarks.py
â”œâ”€â”€ train_model.py
â”œâ”€â”€ realtime_predict.py
â”œâ”€â”€ X.npy
â”œâ”€â”€ y.npy
â”‚
â””â”€â”€ dataset/
    â”œâ”€â”€ A/
    â”œâ”€â”€ B/
    â”œâ”€â”€ C/
```

---

### 3ï¸âƒ£ Extract Hand Landmarks

```bash
python extract_landmarks.py
```

This step generates:

* `X.npy` â†’ feature vectors
* `y.npy` â†’ labels

---

### 4ï¸âƒ£ Train the Model

```bash
python train_model.py
```

The trained model is saved for later use.

---

### 5ï¸âƒ£ Run Real-Time Recognition

```bash
python realtime_predict.py
```

The webcam will open and display the predicted sign in real time.

---

## ğŸ“Š Results

* Accurate recognition of static hand signs
* Fast real-time performance
* Efficient landmark-based representation
* Works best with clear hand visibility and good lighting

---

## âš ï¸ Limitations

* Supports **static signs only**
* Single-hand recognition
* No sentence-level translation
* Performance depends on dataset quality and lighting conditions

---

## ğŸš€ Future Improvements

* Dynamic sign recognition using LSTM
* Word and sentence-level translation
* Arabic Sign Language expansion
* Mobile or web deployment
* Multi-hand recognition

---


## ğŸ”— Links



* **Dataset:**
  ğŸ‘‰ https://www.kaggle.com/datasets/grassknoted/asl-alphabet

---

## ğŸ“Œ Conclusion

This project demonstrates how computer vision and deep learning can be combined to build an efficient sign language recognition system.
It serves as a strong foundation for more advanced research and real-world applications in assistive technologies.

---
